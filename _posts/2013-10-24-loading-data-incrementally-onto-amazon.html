---
layout: post
title: Loading data (incrementally) into Amazon Redshift.
date: '2013-10-24T08:15:00.002-07:00'
author: Dat Le
tags: 
modified_time: '2013-12-20T08:42:30.874-08:00'
thumbnail: http://1.bp.blogspot.com/-uTVYKGqQMGM/UmlWg_AELqI/AAAAAAAAB5g/UM_x0VH9pCM/s72-c/Screen+Shot+2013-10-25+at+1.18.25.png
blogger_id: tag:blogger.com,1999:blog-3261104696526323937.post-2877256632204053574
blogger_orig_url: http://lenguyenthedat.blogspot.com/2013/10/loading-data-incrementally-onto-amazon.html
---

<span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">At Zalora, we (Data Science team) use <a href="http://aws.amazon.com/redshift/">Amazon Redshift</a>&nbsp;as our central data warehouse. Our Redshift cluster stores everything that you can think of, for example:</span><br /><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Current and Historical Product Metadata</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Sale orders, sale items data.</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">User on-site interactions, click-stream events, mailing events, offline events...</span></li></ul><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-uTVYKGqQMGM/UmlWg_AELqI/AAAAAAAAB5g/UM_x0VH9pCM/s1600/Screen+Shot+2013-10-25+at+1.18.25.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img alt="Amazon Redshift @ Zalora" border="0" height="400" src="http://1.bp.blogspot.com/-uTVYKGqQMGM/UmlWg_AELqI/AAAAAAAAB5g/UM_x0VH9pCM/s640/Screen+Shot+2013-10-25+at+1.18.25.png" title="Amazon Redshift @ Zalora" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Amazon Redshift @ Zalora</td></tr></tbody></table><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;"><br />As a result, one of the tasks we need to do is: To load data from various FTP sources into Amazon Redshift everyday.&nbsp;</span><br /><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">As a side note, loading data into Redshift is as simple as it is, all you need is:</span><br /><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">An <a href="http://aws.amazon.com/s3/">Amazon S3</a> Bucket that's at the same region as your Redshift Instance.</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Postgresql command line tool: <a href="http://www.postgresql.org/docs/8.0/static/app-psql.html">PSQL</a> (sudo apt-get install postgresql-client)</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Redshift's <a href="http://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html">COPY</a> command to import data from S3</span><span style="font-family: 'Helvetica Neue', Arial, Helvetica, sans-serif;">&nbsp;</span></li></ul><span style="font-family: 'Helvetica Neue', Arial, Helvetica, sans-serif;">However, for this particular task, there are 3 important criterias, in which, most of the time, only 2 are satisfied:</span><br /><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">FAST: Data need to be consumed as FAST as possible. It's generally ok to just load the entire table all over again everyday, if it's small enough.</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">SIMPLE: There is no ETL tool for the task, thus we need to have an easily maintainable, understandable solution with minimal number of lines of code.</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">SAFE: In short: Fault Tolerance. &nbsp;What if a load fail one day? &nbsp;What if data does not arrive as expected? &nbsp;What if we have duplicated data load?</span></li></ul><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;"><br />So, here is how this task was done:</span></div><div><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Programming Language of choice: Unix Shell Script.</span></li></ul><blockquote class="tr_bq"><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">The reason is simple, with all the available command-line tools:</span><br /><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">- <a href="http://lftp.yar.ru/">LFTP</a> for a 1-liner FTP mirroring task.</span><br /><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">- <a href="http://s3tools.org/s3cmd">S3cmd</a> for a 1-liner S3 mirroring / download / upload task.</span><br /><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">-&nbsp;<a href="http://www.postgresql.org/docs/8.0/static/app-psql.html">PSQL</a> (note that currently, Redshift is based on PostgreSQL 8.0.2) for executing sql commands on Redshift.&nbsp;</span></blockquote></div><div><div><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Infrastructure: A single <a href="http://aws.amazon.com/ec2/">Amazon EC2</a> instance (preferably EBS backed, in the same Region as Redshift Cluster) connecting to everything else:</span></li></ul></div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/--JkfyvrJTeU/UmlULgOrfcI/AAAAAAAAB5I/j7KAQkR4LTw/s1600/Screen+Shot+2013-10-25+at+1.08.36.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="326" src="http://4.bp.blogspot.com/--JkfyvrJTeU/UmlULgOrfcI/AAAAAAAAB5I/j7KAQkR4LTw/s640/Screen+Shot+2013-10-25+at+1.08.36.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Infrastructure Overview</td></tr></tbody></table></div><div><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">The task are functionally broken down into 5 steps:</span></li></ul><blockquote class="tr_bq"><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">LFTP mirroring from FTP Servers into EC2's EBS:</span></li></ul></blockquote><ul><ol></ol></ul><div><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; line-height: 20px; overflow: auto; padding: 0px; width: 646.46875px;"><code style="word-wrap: normal;"><span style="font-family: inherit;">lftp -e "mirror -n $SOURCE $DEST;exit" -u $USER,$PASS $HOST</span></code></pre></div><blockquote class="tr_bq"><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Process data on EC2's EBS (Unzip, Rename if needed, Gzip all files)</span></li></ul></blockquote><ul><ol></ol></ul><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; width: 646.46875px;"><span style="font-family: inherit;"><span style="background-color: transparent; line-height: 20px;">unzip_and_gzip() { #</span><span style="background-color: transparent; line-height: 20px;">$1: filename excluding ALL extensions.</span><span style="line-height: 20px;"><br />    </span><span style="line-height: 20px;"> zipfile=$1.csv.zip</span><span style="line-height: 20px;"><br />     </span><span style="line-height: 20px;">gzipfile=$1.csv.gz</span><span style="line-height: 20px;"><br />     </span><span style="line-height: 20px;">if [ ! -f $gzipfile ]; then # not available then proceed</span><span style="line-height: 20px;"><br />         </span><span style="line-height: 20px;">unzip $1.csv.zip</span><span style="line-height: 20px;"><br /></span><span style="line-height: 20px;">         </span><span style="line-height: 20px;">gzip $1.csv # gzip automatically remove csv file after done</span><span style="line-height: 20px;"><br /></span><span style="line-height: 20px;">    fi<br /> }</span></span></pre><blockquote class="tr_bq"><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">S3cmd sync Gzipped files from EC2's EBS to S3.</span></li></ul></blockquote><ul></ul><div><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; line-height: 20px; overflow: auto; padding: 0px; width: 646.46875px;"><code style="word-wrap: normal;"><span style="font-family: inherit;">s3cmd --config=$CONF sync --skip-existing $SOURCE $DEST</span></code></pre></div><blockquote class="tr_bq"><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Drop and Re-Create Table Definition for each of the Data Source.</span></li></ul></blockquote><ul></ul><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; width: 646.46875px;"><span style="font-family: inherit;"><span style="line-height: 20px;">psql -h $PG_ENDPOINT -U $PG_USER -d $PG_DB -p $PG_PORT -c "<br />    DROP TABLE $SCHEMA.$TABLE CASCADE;"</span><span style="line-height: 20px; white-space: normal;">&nbsp;</span></span></pre><ul></ul><div><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; width: 646.46875px;"><span style="font-family: inherit;"><span style="line-height: 20px;">psql -h $PG_ENDPOINT -U $PG_USER -d $PG_DB -p $PG_PORT -c "<br />    CREATE TABLE $SCHEMA.$TABLE<br />    (col1 varchar(25), <br />     col2 date,<br />     col3 integer,<br />     col4 integer,<br />     col5 timestamp) DISTKEY (col1) SORTKEY (col2, col3);"</span><span style="line-height: 20px; white-space: normal;">&nbsp;</span></span></pre></div><div><blockquote class="tr_bq"><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">COPY data from S3 into each Table on Redshift.</span></li></ul></blockquote><ul></ul></div><div><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; width: 646.46875px;"><span style="font-family: inherit;"><span style="line-height: 20px;">psql -h $PG_ENDPOINT -U $PG_USER -d $PG_DB -p $PG_PORT -c "<br />    COPY $SCHEMA.$TABLE FROM '$S3' <br /></span><span style="line-height: 20px;">    CREDENTIALS 'aws_access_key_id=$ACCESSKEY;aws_secret_access_key=$SECRETKEY'</span><span style="line-height: 20px;"> <br />    COMPUPDATE OFF<br />    EMPTYASNULL<br />    ACCEPTANYDATE<br />    ACCEPTINVCHARS AS '^'<br />    GZIP<br />    TRUNCATECOLUMNS<br />    FILLRECORD<br />    DELIMITER '$DELIM'<br />    REMOVEQUOTES<br />    STATUPDATE ON<br />    MAXERROR AS $MaxERROR;"</span></span></pre><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;"><br /></span></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">With these processes, "SIMPLE" and "SAFE" are more or less satisfied. All data are backed up on to S3 as well as EBS, no data deletion along the process. If needed, just fire off either LFTP, S3 sync or psql COPY command and data will be loaded into Redshift easily, otherwise, all tables will be complete re-created / fixed automatically the next day.</span></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;"><br /></span></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Everything seems easy enough, however, here comes the problem: FAST. As it turns out, some of the tables take more than 10 hours to load into Redshift, which means, re-creating the table simply doesn't work for us anymore, incrementally load is needed.</span></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;"><br /></span></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">The main question is: "How do we know which data is already loaded, which data is new and needs to be loaded into Redshift?"</span></div><div><ul><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Removing loaded data doesn't help, one of the requirements is to keep source data 100% untouched and retained on the cloud.</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Moving "loaded" data into other location seems ok, but it doesn't help either. LFTP and S3Sync will still mirror everything fully back.</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Load data by date: data arrives to our FTP location can be delayed, loading it by date means we will have to deal with the problem of missing data, data duplication. Oh and and also, timezone problem.</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Store loaded data files and use them as a filter when loading into S3 or Redshift. This seems like a feasible solution, but programming wise, it is going to be more complicated and introduces unnecessary dependency: new location for storing load-history, proper data structure for load-history...</span></li></ul></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">As a result, a simple "hack" is introduced:&nbsp;</span></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">- All gzipped files are truncated / replaced into / by empty gzipped files after loaded into Redshift.</span></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">- Two S3 locations instead of just one, are used: one for incremental loading (refreshed everyday with all data, containing dummy gzipped files that were already loaded), one for backing up (only load in new data).</span></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;"><br /></span></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">In the end:&nbsp;</span></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">- Original zipped files are retained.</span><br /><span style="font-family: 'Helvetica Neue', Arial, Helvetica, sans-serif;">- Load data can be done incrementally, regardless of the time when they were exported. As a result, a 10 hours loading process is now becoming 5 minutes process.</span></div><div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">- 100% uptime since no "Dropping &amp; Re-creating" task needed.</span></div><div></div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">- Programming wise: nothing but a simple bash function is needed to be called after each loading job:</span></div><div><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; width: 646.46875px;"><span style="font-family: inherit; line-height: 20px;">create_dummy_gz_file () {<br />    rm -f $DATA/*.gz<br />    zipped_files_list="$(ls $DATA/*.zip 2&gt;/dev/null)"<br />    for f in $zipped_files_list;<br />        do<br />            filename=`echo "$f" | cut -d'.' -f1` # anything before the dot '.'<br />            touch "$filename".csv &amp;<br />        done <br />        wait<br />        gzip $Data/*.csv<br />}</span></pre></div><div><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;"><b>TL;DR:</b></span><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-lRJA8z9EJ_M/UmlUVJ1GdnI/AAAAAAAAB5Y/v5W6rgkg7is/s1600/Screen+Shot+2013-10-25+at+1.08.28.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="394" src="http://1.bp.blogspot.com/-lRJA8z9EJ_M/UmlUVJ1GdnI/AAAAAAAAB5Y/v5W6rgkg7is/s640/Screen+Shot+2013-10-25+at+1.08.28.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Infrastructure Overview</td></tr></tbody></table><ol><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Amazon EC2(s) with EBS.</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Bash scripting with lftp, psql, s3cmd.</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">Functional design.</span></li><li><span style="font-family: Helvetica Neue, Arial, Helvetica, sans-serif;">For incremental load: dummy gzipped files created after original files are loaded.</span></li></ol></div><div></div></div></div>