---
layout: post
title: 'Minimal Data Science #1: Classify StarCraft 2 players with Python Pandas and
  Scikit-learn.'
date: '2014-07-21T06:56:00.001-07:00'
author: Dat Le
tags: 
modified_time: '2014-07-29T08:52:42.374-07:00'
thumbnail: http://3.bp.blogspot.com/-c57m4J8cSQc/U80-Gyo19WI/AAAAAAAADJE/AUJrG6zWp9U/s72-c/Screen+Shot+2014-07-21+at+21.47.47.png
blogger_id: tag:blogger.com,1999:blog-3261104696526323937.post-3295306444867294091
blogger_orig_url: http://lenguyenthedat.blogspot.com/2014/07/minimal-data-science-1-analyze.html
---

<b>Forewords:</b> This is the first in a data science blog series that I'm writing. My goal for this series is not only sharing, tutorializing, but also, making personal notes while learning and working as a Data Scientist. I'm looking forward to receiving any feedback from you.<br /><br />Note: The source codes as well as original datasets for this series will also be updated at this <a href="https://github.com/lenguyenthedat/minimal-datascience">Github repository</a>&nbsp;of mine.<br /><br />For the purpose of this project, I'm going to use this&nbsp;<a href="http://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset">SkillCraft</a> data set.<br />(You can also download as well as have a quick look of it from this&nbsp;<a href="http://www.sharecsv.com/s/dd4eadbc6e0632dc820b0c017d82aa80/SkillCraft1_Dataset.csv">ShareCSV url</a>&nbsp; - Many thanks to <a href="https://github.com/kentran">Ken Tran</a> and <a href="https://github.com/nvquanghuy">Huy Nguyen</a> for such a neat tool!)<br /><br /><b>Input:</b> &nbsp;StarCraft 2 dataset (CSV) with 20 different attributes.<br /><b><br /></b><b>Output:</b> A classification / prediction model to determine League Index (more information and context can be found&nbsp;<a href="http://wiki.teamliquid.net/starcraft2/Battle.net_Leagues">here</a>).<br /><b><br /></b><b>Prerequisites:</b> Python (with <a href="http://pandas.pydata.org/">Pandas</a> and <a href="http://scikit-learn.org/">Scikit-learn</a>), <a href="http://ipython.org/notebook.html">iPython Notebook</a>&nbsp;(as an awesome IDE), Unix Shell Script.<br /><br /><b>Step 1</b>: Clean the data. I'm going to use a simple Bash script for this.<br /><br />As you can see: there is some "missing-value" entries - denoted as "?":<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">$ cat SkillCraft1_Dataset.csv | grep "?" | head -2<br />1064,5,17,20,"?",94.4724,0.0038460052,0.0007827297,3,"9.66332959684589e-06",0.0001352866,0.0044741216,50.5455,54.9287,3.0972,31,0.0007634,7,0.0001062966,0.00011596<br />5255,5,18,"?","?",122.247,0.0063568492,0.0004328068,3,"1.35252109932915e-05",0.000256979,0.0030431725,30.8929,62.2933,5.3822,23,0.001055,5,0,0.00033813</span></span></pre><br />A 1-liner bash script to clean them up:<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">$ sed '/?/d' SkillCraft1_Dataset.csv &gt; SkillCraft1_Dataset_clean.csv</span></span></pre><br /><b>Step 2</b>: Load, prepare the dataset. We will use iPython Notebook from now.<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">import pandas as pd<br />df = pd.read_csv('./Dataset/Starcraft/SkillCraft1_Dataset_clean.csv')</span></span></pre><br />Divide the given data set into Train (75%) and Test (25%) sets:<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">df['is_train'] = np.random.uniform(0, 1, len(df)) &lt;= .75<br />train_df, test_df = df[df['is_train']==True], df[df['is_train']==False]</span></span></pre><div><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;"><br /></span></span></div><b>Step 3:</b>&nbsp;Apply Machine Learning algorithm.<br /><br />We will use <a href="http://en.wikipedia.org/wiki/Random_forest">Random Forest</a> classification for this example:<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">from sklearn.ensemble import RandomForestClassifier<br /></span></span></pre><div><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;"><br /></span></span></div>Chose the features set:<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">features = ["APM","Age","TotalHours","UniqueHotkeys", "SelectByHotkeys", "AssignToHotkeys", "WorkersMade","ComplexAbilitiesUsed","MinimapAttacks","MinimapRightClicks"]</span></span><br /></pre><div style="text-align: justify;"><br /></div>Define your classifier and train it:<br />(n_estimators is the number of trees in your forest, in this case I would just use 10.)<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">rfc = RandomForestClassifier(n_estimators=10)<br />rfc.fit(train_df[list(features)], train_df.LeagueIndex)</span></span><br /></pre><div style="text-align: justify;"><br /></div><b>Step 4:</b> Evaluations:<br /><br /><a href="http://en.wikipedia.org/wiki/Contingency_table">Contingency Table</a>:<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">pd.crosstab(test_df.LeagueIndex, rfc.predict(test_df[features]), rownames=["Pred"], colnames=["Actual"])</span></span></pre><div class="separator" style="clear: both; text-align: left;"><br /></div><div class="separator" style="clear: both; text-align: left;">(We can see that the correctness reduce greatly at League #1 and #7. It's simply because they are the 2 classes with lowest number of members.</div><div class="separator" style="clear: both; text-align: left;">With a little over 3000 entries in our given dataset, we simply don't have enough data to "learn".)</div><div class="separator" style="clear: both; text-align: center;"></div><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-c57m4J8cSQc/U80-Gyo19WI/AAAAAAAADJE/AUJrG6zWp9U/s1600/Screen+Shot+2014-07-21+at+21.47.47.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://3.bp.blogspot.com/-c57m4J8cSQc/U80-Gyo19WI/AAAAAAAADJE/AUJrG6zWp9U/s1600/Screen+Shot+2014-07-21+at+21.47.47.png" height="315" width="320" /></a></div><br /><br />Plot it!<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">pd.crosstab(test_df.LeagueIndex, rfc.predict(test_df[features]), rownames=["Pred"], colnames=["Actual"]).plot()</span></span></pre><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-sqr28hLbjts/U80Z7iiC2_I/AAAAAAAADIs/TZwtf5du418/s1600/Screen+Shot+2014-07-21+at+21.47.09.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-sqr28hLbjts/U80Z7iiC2_I/AAAAAAAADIs/TZwtf5du418/s1600/Screen+Shot+2014-07-21+at+21.47.09.png" height="231" width="320" /></a></div><div><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;"><br /></span></span></div><a href="http://en.wikipedia.org/wiki/Root-mean-square_deviation">RMSD</a>:<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">np.sqrt(sum(pow(test_df.LeagueIndex - classifier.predict(test_df[features]),2)) / float(len(test_df)))</span></span></pre><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">&gt; </span></span><span style="background-color: white; font-family: monospace, sans-serif; font-size: 14.44444465637207px; line-height: 19.559223175048828px; white-space: pre-wrap;">1.23877447987</span></pre><br /><b><br /></b><b>Conclusions:</b><br />- With the above (relatively confident) result, we can already see the benefit of using data science in a simple classification problem.<br />- There are, of course, a few ways to improve our result:<br /><ol><li>Larger dataset. 3000 entries is simply not enough for this model.</li><li>Better dataset. We would love to get a dataset with an even distribution of League members.</li><li>For Random Forest, simply improving our features set in quality and quantity, or increasing the number of trees would give us a better result at a certain performance cost.</li><li>Consider different classifiers (described below).</li></ol><b></b><br /><div><b><b><br /></b></b></div><b>Take it to another level:</b><br /><div><div>Now that we've used a proper classification technique (Random Forest), but that's not everything about data science. Choosing the right technique for the right task is not only an interesting problem, but also mandatory.</div><div>In this part, we are going to compare between various classifiers and pick the best one for the task:<br /><br />Mass-define classifiers:<br /><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier<br />from sklearn.neighbors import KNeighborsClassifier<br />from sklearn.lda import LDA<br />from sklearn.qda import QDA<br />from sklearn.naive_bayes import GaussianNB<br />from sklearn.tree import DecisionTreeClassifier<br /><br />classifiers = [ <br />    ExtraTreesClassifier(n_estimators=10),<br />    RandomForestClassifier(n_estimators=10),<br />    KNeighborsClassifier(100),<br />    LDA(),<br />    QDA(),<br />    GaussianNB(),<br />    DecisionTreeClassifier()<br />]</span></span></pre></div></div><div><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;"><br /></span></span></div><div>Train them:</div><div><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">import time<br />features = ["APM","Age","TotalHours","UniqueHotkeys", "SelectByHotkeys", "AssignToHotkeys",<br />            "WorkersMade","ComplexAbilitiesUsed","MinimapAttacks","MinimapRightClicks" ]<br /><br />for classifier in classifiers:<br />    print classifier.__class__.__name__<br />    start = time.time()<br />    classifier.fit(train_df[list(features)], train_df.LeagueIndex)<br />    print "  -&gt; Training time:", time.time() - start</span></span><br /></pre></div><div class="separator" style="clear: both; text-align: center;"></div><div style="text-align: justify;"><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-Ecbjj5vBPRs/U9fCVN9pnqI/AAAAAAAADKM/42rbh7W1X5c/s1600/Screen+Shot+2014-07-29+at+23.50.19.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><br /></a><a href="http://4.bp.blogspot.com/-Ecbjj5vBPRs/U9fCVN9pnqI/AAAAAAAADKM/42rbh7W1X5c/s1600/Screen+Shot+2014-07-29+at+23.50.19.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-Ecbjj5vBPRs/U9fCVN9pnqI/AAAAAAAADKM/42rbh7W1X5c/s1600/Screen+Shot+2014-07-29+at+23.50.19.png" height="238" width="320" /></a></div><br /></div><div><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />Evaluation:</div><div><pre style="background-color: #f0f0f0; border: 1px dashed rgb(204, 204, 204); height: auto; overflow: auto; padding: 0px; text-align: justify; width: 646.46875px;"><span style="color: #333333;"><span style="font-size: 14px; line-height: 20px;">for classifier in classifiers:<br />    print classifier.__class__.__name__<br />    </span></span><span style="background-color: transparent; font-size: 14px; line-height: 20px;"><span style="color: #333333;">print np.sqrt(sum(pow(test_df.LeagueIndex - classifier.predict(test_df[features]),2)) / float(len(test_df)))</span></span></pre></div><div class="separator" style="clear: both; text-align: center;"></div><div><div class="separator" style="clear: both; text-align: center;"></div></div><div><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-AUCd2P0B-0Y/U9fDFQAzsOI/AAAAAAAADKU/nYcfca9jUsE/s1600/Screen+Shot+2014-07-29+at+23.50.05.png" imageanchor="1" style="clear: left; float: left; margin-bottom: 1em; margin-right: 1em;"><img border="0" src="http://3.bp.blogspot.com/-AUCd2P0B-0Y/U9fDFQAzsOI/AAAAAAAADKU/nYcfca9jUsE/s1600/Screen+Shot+2014-07-29+at+23.50.05.png" height="235" width="320" /></a></div>As we can see,&nbsp;<a href="http://en.wikipedia.org/wiki/Linear_discriminant_analysis">Linear Discriminant Analysis</a> (LDA) clearly won with a relatively low training time and best RMSD!</div>